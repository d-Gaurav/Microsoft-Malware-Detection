import time
import json

import numpy as np
import pandas as pd

chunk_size = 4000
aggregate = dict()


def normalize(feature_file_path, output_file_path, data_agg):
    """
    Normalizes all the features between a range of 0-1
    :param feature_file_path: str
    :param output_file_path: str
    :param data_agg: pandas.DataFrame
    :return bool
    """
    header = True
    idx_chunk = 0
    for bytes_feature in pd.read_csv(bytes_feature_file_path, chunksize=chunk_size):
        if idx_chunk == 0:
            columns = [col for col in bytes_feature.columns if col not in [
                'index', 'Unnamed: 0']]
            idx_chunk += 1
        for column in columns:
            bytes_feature[column] = ((bytes_feature[column] - data_agg[column]['min']) / (
                data_agg[column]['max'] - data_agg[column]['min'])).astype('int64', copy=False  )

        bytes_feature.to_csv(output_file_path,
                             header=header, mode='a')
        header = False
    return True


def generate_agg(data, columns, file_path=None):
    global aggregate
    for column in columns:
        if not aggregate.get(column):
            col_agg = dict()
            col_agg['min'] = data[column].min()
            col_agg['max'] = data[column].max()
            aggregate[column] = col_agg
        else:
            col_agg = aggregate.get(column)
            current_data_min = data[column].min()
            current_data_max = data[column].max()
            col_agg['min'] = current_data_min if col_agg['min'] > current_data_min else col_agg['min']
            col_agg['max'] = current_data_max if col_agg['max'] < current_data_max else col_agg['max']
    if file_path:
        aggregate.to_json(file_path)


def main(bytes_feature_file_path, byte_labels, drive_path):
    byte_labels_df = pd.read_csv(byte_labels)
    byte_labels_df['index'] = byte_labels_df['ID'] + '.txt'
    idx_chunk = 0
    header = True
    for bytes_feature in pd.read_csv(bytes_feature_file_path, chunksize=chunk_size):
        bytes_feature = bytes_feature.reset_index()
        if idx_chunk == 0:
            columns = [col for col in bytes_feature.columns if col not in [
                'index', 'Unnamed: 0']]
            idx_chunk += 1
        bytes_feature = bytes_feature.merge(
            byte_labels_df[['size', 'index']], on='index', how='left')
        bytes_feature.to_csv(drive_path+'bytes_features_with_size.csv',
                             header=header, mode='a')
        header = False
        generate_agg(bytes_feature, columns, file_path=None)
    global aggregate
    aggregate = pd.DataFrame(aggregate)
    aggregate.to_csv(drive_path+'bytes_feature_agg.csv')
    normalized = normalize(drive_path+'bytes_features_with_size.csv',
                           drive_path+'bytes_feature_normalized.csv', aggregate)
    print(f"Data Normalized - {normalized}")


if __name__ == "__main__":
    drive_path = ''
    bytes_feature_file_path = 'bytes_feature_op.csv'
    byte_labels = 'train_labels_with_byte_size.csv'
    start = time.time()
    main(bytes_feature_file_path, byte_labels, drive_path)
    print(f"Execution completed in {time.time() - start} secs.")
